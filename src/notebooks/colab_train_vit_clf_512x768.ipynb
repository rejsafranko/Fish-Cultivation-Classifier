{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_v1w-cvU0Dm"
      },
      "source": [
        "Environment setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDkuPUDuHsq8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4mNZpCfVR9c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA_JjJCrVYTY"
      },
      "outputs": [],
      "source": [
        "colab_data_path = \"/content/drive/MyDrive/Seminar2/data/ribe_512x768/\"\n",
        "colab_dir = \"/content/drive/MyDrive/Seminar2/model/\"\n",
        "model_name_or_path = \"google/vit-base-patch16-224-in21k\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JYv5xzZRFogJ"
      },
      "source": [
        "Imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZRJkBtHEFogM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import (\n",
        "    ViTForImageClassification,\n",
        "    ViTFeatureExtractor,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aA9WGxqJVU-3"
      },
      "source": [
        "Prepare dataset splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhi8DSzITmdl"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"imagefolder\", data_dir=colab_data_path)\n",
        "splits = dataset[\"train\"].train_test_split(test_size=0.33)\n",
        "dataset[\"train\"] = splits[\"train\"]\n",
        "dataset[\"val\"] = splits[\"test\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kwwjvgdiUmBo"
      },
      "source": [
        "Define image augmentations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA5mIx_OToRi"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0WakqvlvUopP"
      },
      "source": [
        "Apply the transforms to the train set. Concatenate the transformed and original train set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlwRRw3WTpuH"
      },
      "outputs": [],
      "source": [
        "transformed_train_dataset = dataset[\"train\"].map(\n",
        "    lambda example: {\"image\": transform(example[\"image\"]), \"label\": example[\"label\"]}\n",
        ")\n",
        "\n",
        "dataset[\"train\"] = concatenate_datasets([transformed_train_dataset, dataset[\"train\"]])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ubKH1L0uUIaZ"
      },
      "source": [
        "Configure the Feature Extractor and apply it to the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt39CjLkTttG"
      },
      "outputs": [],
      "source": [
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    do_resize=False,\n",
        "    do_rescale=False,\n",
        "    patch_size=64,\n",
        ")\n",
        "\n",
        "def transform(example_batch):\n",
        "    inputs = feature_extractor(\n",
        "        [x.convert(\"RGB\") for x in example_batch[\"image\"]], return_tensors=\"pt\"\n",
        "      )\n",
        "    inputs[\"labels\"] = example_batch[\"label\"]\n",
        "    return inputs\n",
        "\n",
        "dataset = dataset.with_transform(transform)\n",
        "labels = dataset['train'].features['label'].names"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v8gCzNfXUFIr"
      },
      "source": [
        "Configure Vision Transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAY_kWjoTxXy"
      },
      "outputs": [],
      "source": [
        "# Set interpolate_pos_encoding=True in the source code.\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels=len(labels),\n",
        "    id2label={str(i): c for i, c in enumerate(labels)},\n",
        "    label2id={c: str(i) for i, c in enumerate(labels)},\n",
        "    ignore_mismatched_sizes=True,\n",
        ")\n",
        "\n",
        "model.config.hidden_dropout_prob = 0.5"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PWDbGDVJUArT"
      },
      "source": [
        "Define auxiliary functions for the training procedure and configure training parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia00I2v0VBBz"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.tensor([x['labels'] for x in batch])\n",
        "    }\n",
        "\n",
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "  preds = np.argmax(eval_pred.predictions, axis=1)\n",
        "  return {\n",
        "    \"acc\": accuracy_score(eval_pred.label_ids, preds),\n",
        "    \"f1\": f1_score(eval_pred.label_ids, preds, average=\"weighted\"),\n",
        "    \"precision\": precision_score(eval_pred.label_ids, preds, average=\"weighted\"),\n",
        "    \"recall\": recall_score(eval_pred.label_ids, preds, average=\"weighted\")\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G9sQ4QQT2Dg"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "  output_dir=\"/content/drive/MyDrive/Seminar2/model\",\n",
        "  per_device_train_batch_size=4,\n",
        "  per_device_eval_batch_size=4,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=6,\n",
        "  fp16=True,\n",
        "  save_steps=60,\n",
        "  eval_steps=60,\n",
        "  warmup_steps=500,\n",
        "  logging_steps=60,\n",
        "  learning_rate=2e-4,\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='tensorboard',\n",
        "  load_best_model_at_end=True,\n",
        "  weight_decay=0.1,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"val\"],\n",
        "    tokenizer=feature_extractor,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LQf8Mfm_T9Be"
      },
      "source": [
        "Train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZukltIaFogV"
      },
      "outputs": [],
      "source": [
        "train_results = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NYVGh6DvT-bb"
      },
      "source": [
        "Evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euTATbBdJSPN"
      },
      "outputs": [],
      "source": [
        "metrics = trainer.evaluate(dataset[\"test\"])\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
